{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Atelier d'apprentissage par renforcement : Agent Q-learning pour le Morpion"
      ],
      "metadata": {
        "id": "_Yy8oGhrKA41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cet atelier, nous allons développer un agent d'apprentissage par renforcement pour jouer au Morpion en utilisant la méthode Q-learning. Nous utiliserons la bibliothèque NumPy pour gérer les tableaux et les calculs."
      ],
      "metadata": {
        "id": "r6t-7aSrKKFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Création de l'environnement de Morpion"
      ],
      "metadata": {
        "id": "J7iDJj8NKNvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AkPO0q-YEH9Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        # Initialize the board (3x3 matrix filled with zeros)\n",
        "        self.board = np.zeros((3, 3))\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the board to its initial state\n",
        "        self.board = np.zeros((3, 3))\n",
        "        return self.board.flatten()\n",
        "\n",
        "    def available_actions(self):\n",
        "        # Get the indices of the available actions (empty cells)\n",
        "        return np.argwhere(self.board == 0)\n",
        "\n",
        "    def step(self, action, player):\n",
        "        # Place the player's marker (1 for player 1, -1 for player 2) at the chosen position\n",
        "        self.board[action] = player\n",
        "\n",
        "        # Check if the player has won\n",
        "        if np.any(np.sum(self.board, axis=0) == 3 * player) or np.any(np.sum(self.board, axis=1) == 3 * player) or np.sum(np.diagonal(self.board)) == 3 * player or np.sum(np.diagonal(np.fliplr(self.board))) == 3 * player:\n",
        "            reward = 1\n",
        "            done = True\n",
        "        # Check if the player has lost\n",
        "        elif np.any(np.sum(self.board, axis=0) == 3 * -player) or np.any(np.sum(self.board, axis=1) == 3 * -player) or np.sum(np.diagonal(self.board)) == 3 * -player or np.sum(np.diagonal(np.fliplr(self.board))) == 3 * -player:\n",
        "            reward = -1\n",
        "            done = True\n",
        "        # Check if the game is a draw\n",
        "        elif np.all(self.board != 0):\n",
        "            reward = 0\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return self.board.flatten(), reward, done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initialisation du Q-table"
      ],
      "metadata": {
        "id": "6koPmtDhKivX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-table initialization\n",
        "state_size = 3**9\n",
        "action_size = 9\n",
        "q_table = np.zeros((state_size, action_size))"
      ],
      "metadata": {
        "id": "OmTfhbBjERIH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configuration des paramètres d'apprentissage et entraînement de l'agent"
      ],
      "metadata": {
        "id": "MG8pGbPlKrPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1  # Exploration rate\n",
        "epsilon_decay = 0.9999\n",
        "min_epsilon = 0.01"
      ],
      "metadata": {
        "id": "Xxj5NOoAK0SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "n_episodes = 50000\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    env = TicTacToe()\n",
        "    state = env.reset()\n",
        "    state_idx = int(''.join(map(str, state.astype(int) + 1)), 3)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose an action (exploration or exploitation)\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = np.random.choice(action_size)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state_idx])\n",
        "\n",
        "        # Take the action and get the next state, reward, and done flag\n",
        "        next_state, reward, done = env.step(np.unravel_index(action, (3, 3)), 1)\n",
        "\n",
        "        # Update the Q-table\n",
        "        next_state_idx = int(''.join(map(str, next_state.astype(int) + 1)), 3)\n",
        "        q_table[state_idx, action] += alpha * (reward + gamma * np.max(q_table[next_state_idx]) - q_table[state_idx, action])\n",
        "\n",
        "        # Update the state and state_idx\n",
        "        state = next_state\n",
        "        state_idx = next_state_idx\n",
        "\n",
        "    # Decay the exploration rate\n",
        "    epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
        "\n",
        "    # Print the episode number and epsilon value every 1000 episodes\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode: {episode}, Epsilon: {epsilon}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viNJ300LESGk",
        "outputId": "5138dc25-a810-4261-beaa-83e6af7412e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Epsilon: 0.9999\n",
            "Episode: 1000, Epsilon: 0.9047424102692004\n",
            "Episode: 2000, Epsilon: 0.8186406930090225\n",
            "Episode: 3000, Epsilon: 0.7407330270401349\n",
            "Episode: 4000, Epsilon: 0.6702396082111141\n",
            "Episode: 5000, Epsilon: 0.6064548440752141\n",
            "Episode: 6000, Epsilon: 0.548740291377179\n",
            "Episode: 7000, Epsilon: 0.4965182656589779\n",
            "Episode: 8000, Epsilon: 0.4492660590208893\n",
            "Episode: 9000, Epsilon: 0.406510708161521\n",
            "Episode: 10000, Epsilon: 0.3678242603283259\n",
            "Episode: 11000, Epsilon: 0.332819489793915\n",
            "Episode: 12000, Epsilon: 0.3011460219829101\n",
            "Episode: 13000, Epsilon: 0.27248682645444433\n",
            "Episode: 14000, Epsilon: 0.24655504363736244\n",
            "Episode: 15000, Epsilon: 0.22309111355585096\n",
            "Episode: 16000, Epsilon: 0.20186017780594118\n",
            "Episode: 17000, Epsilon: 0.1826497287783945\n",
            "Episode: 18000, Epsilon: 0.16526748259824006\n",
            "Episode: 19000, Epsilon: 0.14953945449050376\n",
            "Episode: 20000, Epsilon: 0.13530821730781062\n",
            "Episode: 21000, Epsilon: 0.12243132578887629\n",
            "Episode: 22000, Epsilon: 0.11077989077575923\n",
            "Episode: 23000, Epsilon: 0.10023728911873117\n",
            "Episode: 24000, Epsilon: 0.09069799635576713\n",
            "Episode: 25000, Epsilon: 0.08206653048255198\n",
            "Episode: 26000, Epsilon: 0.0742564962408389\n",
            "Episode: 27000, Epsilon: 0.0671897203591181\n",
            "Episode: 28000, Epsilon: 0.06079546908993113\n",
            "Episode: 29000, Epsilon: 0.0550097402118921\n",
            "Episode: 30000, Epsilon: 0.049774622409830716\n",
            "Episode: 31000, Epsilon: 0.04503771562087175\n",
            "Episode: 32000, Epsilon: 0.04075160654450075\n",
            "Episode: 33000, Epsilon: 0.03687339406682043\n",
            "Episode: 34000, Epsilon: 0.03336425984880598\n",
            "Episode: 35000, Epsilon: 0.03018907978043465\n",
            "Episode: 36000, Epsilon: 0.02731607241160077\n",
            "Episode: 37000, Epsilon: 0.024716480840843772\n",
            "Episode: 38000, Epsilon: 0.022364284877805257\n",
            "Episode: 39000, Epsilon: 0.020235940598352414\n",
            "Episode: 40000, Epsilon: 0.01831014468548625\n",
            "Episode: 41000, Epsilon: 0.016567621197244295\n",
            "Episode: 42000, Epsilon: 0.014990928627284598\n",
            "Episode: 43000, Epsilon: 0.01356428532695572\n",
            "Episode: 44000, Epsilon: 0.012273411541443215\n",
            "Episode: 45000, Epsilon: 0.01110538647887905\n",
            "Episode: 46000, Epsilon: 0.010048518981770194\n",
            "Episode: 47000, Epsilon: 0.01\n",
            "Episode: 48000, Epsilon: 0.01\n",
            "Episode: 49000, Epsilon: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Test de l'agent entraîné"
      ],
      "metadata": {
        "id": "cPqm0SM2LAh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_test_episodes = 100\n",
        "wins = 0\n",
        "draws = 0\n",
        "losses = 0\n",
        "\n",
        "for episode in range(n_test_episodes):\n",
        "    env = TicTacToe()\n",
        "    state = env.reset()\n",
        "    state_idx = int(''.join(map(str, state.astype(int) + 1)), 3)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose the best action according to the Q-table\n",
        "        action = np.argmax(q_table[state_idx])\n",
        "\n",
        "        # Take the action and get the next state, reward, and done flag\n",
        "        next_state, reward, done = env.step(np.unravel_index(action, (3, 3)), 1)\n",
        "\n",
        "        # Update the state and state_idx\n",
        "        state = next_state\n",
        "        state_idx = int(''.join(map(str, state.astype(int) + 1)), 3)\n",
        "\n",
        "        if not done:\n",
        "            # Random opponent's turn\n",
        "            opponent_action = np.random.choice(len(env.available_actions()))\n",
        "            next_state, _, done = env.step(env.available_actions()[opponent_action], -1)\n",
        "\n",
        "            # Update the state and state_idx\n",
        "            state = next_state\n",
        "            state_idx = int(''.join(map(str, state.astype(int) + 1)), 3)\n",
        "\n",
        "    if reward == 1:\n",
        "        wins += 1\n",
        "    elif reward == 0:\n",
        "        draws += 1\n",
        "    else:\n",
        "        losses += 1\n",
        "\n",
        "print(f\"Wins: {wins}, Draws: {draws}, Losses: {losses}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Jv2w9QEn0x",
        "outputId": "c5cf5a18-f0c8-43ea-c7b0-614c7a56d7be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wins: 0, Draws: 100, Losses: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Jouer contre l'agent"
      ],
      "metadata": {
        "id": "L_DFEGa0Li7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fonction d'affichage du plateau de jeu\n",
        "def render(board):\n",
        "    symbols = {0: ' ', 1: 'X', -1: 'O'}\n",
        "    print('---------')\n",
        "    for i in range(3):\n",
        "        print('|', end='')\n",
        "        for j in range(3):\n",
        "            print(symbols[board[i, j]], end='|')\n",
        "        print('\\n---------')\n"
      ],
      "metadata": {
        "id": "20Wfa9J4E4_2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_winner(board):\n",
        "    for player in [1, -1]:\n",
        "        if np.any(np.sum(board, axis=0) == 3 * player) or np.any(np.sum(board, axis=1) == 3 * player) or np.sum(np.diagonal(board)) == 3 * player or np.sum(np.diagonal(np.fliplr(board))) == 3 * player:\n",
        "            return player\n",
        "    return None\n",
        "\n",
        "def valid_move(board, action):\n",
        "    return board[np.unravel_index(action, (3, 3))] == 0\n",
        "\n",
        "env = TicTacToe()\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # Agent's turn\n",
        "    state_idx = int(''.join(map(str, state.astype(int) + 1)), 3)\n",
        "    \n",
        "    # Make sure the agent picks a valid move\n",
        "    valid_actions = np.where(q_table[state_idx] != 0)[0]\n",
        "    \n",
        "    if valid_actions.size > 0:\n",
        "        action = valid_actions[np.argmax(q_table[state_idx, valid_actions])]\n",
        "    else:\n",
        "        action = np.random.choice(np.ravel_multi_index(env.available_actions().T, (3, 3)))\n",
        "    \n",
        "    if not valid_move(env.board, action):\n",
        "        action = np.random.choice(np.ravel_multi_index(env.available_actions().T, (3, 3)))\n",
        "    \n",
        "    state, _, done = env.step(np.unravel_index(action, (3, 3)), 1)\n",
        "    \n",
        "    render(env.board)\n",
        "    \n",
        "    winner = check_winner(env.board)\n",
        "    if winner is not None:\n",
        "        break\n",
        "    \n",
        "    if not done:\n",
        "        # Player's turn\n",
        "        player_action = int(input(\"Enter your move (0-8): \"))\n",
        "        state, _, done = env.step(np.unravel_index(player_action, (3, 3)), -1)\n",
        "\n",
        "        render(env.board)\n",
        "\n",
        "        winner = check_winner(env.board)\n",
        "        if winner is not None:\n",
        "            break\n",
        "\n",
        "# Display the game result\n",
        "if winner == 1:\n",
        "    print(\"You lost!\")\n",
        "elif winner == -1:\n",
        "    print(\"You won!\")\n",
        "else:\n",
        "    print(\"It's a draw!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_Z65vfrFPhN",
        "outputId": "7022eb5f-779e-45a6-b2a8-f19a08bba2db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "|X| | |\n",
            "---------\n",
            "| | | |\n",
            "---------\n",
            "| | | |\n",
            "---------\n",
            "Enter your move (0-8): 1\n",
            "---------\n",
            "|X|O| |\n",
            "---------\n",
            "| | | |\n",
            "---------\n",
            "| | | |\n",
            "---------\n",
            "---------\n",
            "|X|O| |\n",
            "---------\n",
            "| | | |\n",
            "---------\n",
            "|X| | |\n",
            "---------\n",
            "Enter your move (0-8): 3\n",
            "---------\n",
            "|X|O| |\n",
            "---------\n",
            "|O| | |\n",
            "---------\n",
            "|X| | |\n",
            "---------\n",
            "---------\n",
            "|X|O| |\n",
            "---------\n",
            "|O| | |\n",
            "---------\n",
            "|X| |X|\n",
            "---------\n",
            "Enter your move (0-8): 7\n",
            "---------\n",
            "|X|O| |\n",
            "---------\n",
            "|O| | |\n",
            "---------\n",
            "|X|O|X|\n",
            "---------\n",
            "---------\n",
            "|X|O| |\n",
            "---------\n",
            "|O| |X|\n",
            "---------\n",
            "|X|O|X|\n",
            "---------\n",
            "Enter your move (0-8): 2\n",
            "---------\n",
            "|X|O|O|\n",
            "---------\n",
            "|O| |X|\n",
            "---------\n",
            "|X|O|X|\n",
            "---------\n",
            "---------\n",
            "|X|O|O|\n",
            "---------\n",
            "|O|X|X|\n",
            "---------\n",
            "|X|O|X|\n",
            "---------\n",
            "You lost!\n"
          ]
        }
      ]
    }
  ]
}